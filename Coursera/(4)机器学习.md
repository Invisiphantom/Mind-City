

# 聚类算法

## K-Means算法

```py
import numpy as np
from scipy.spatial import distance
from sklearn.cluster import KMeans

import plotly.graph_objects as go
from plotly.subplots import make_subplots


def build_kpp_centers(k, X):
    """构建k-Means++初始中心点集"""

    # 选取初始中心点
    cents = [X[np.random.choice(range(X.shape[0]))]]

    sumDst = 0
    dists = np.zeros(X.shape[0])
    for _ in range(1, k):

        # 统计每个样本点到最近中心点的距离平方
        for pI, p in enumerate(X):
            _, pDst = closest_center(p, cents)
            dists[pI] = pDst**2
            sumDst += pDst**2

        # 轮盘法选出下一个聚类中心
        isTest = False
        sumDst = sumDst * np.random.random()
        for pI, pDst in enumerate(dists):
            sumDst -= pDst
            # 选中该点, 并跳出循环
            if sumDst <= 0:
                isTest = True 
                cents.append(X[pI])
                sumDst = 0
                break

        assert isTest == True
    return np.array(cents)


def closest_center(p, cents):
    """计算距离当前样本最近的中心点"""
    minI, minDst = 0, float("inf")
    for cI, c in enumerate(cents):
        dist = distance.euclidean(p, c)
        if dist < minDst:
            minI = cI
            minDst = dist
    return minI, minDst


def Mykmeans(X, k, max_times, isKpp=False):
    """K-Means聚类算法"""

    # 如果是传统算法, 直接选取k个非重复点
    if isKpp == False:
        cents = X[np.random.choice(range(X.shape[0]), k, replace=False)]
    # 如果是kmeans++, 则选取k个分散点
    else:
        cents = build_kpp_centers(k, X)

    for t in range(max_times):
        print(f"第{t}次迭代", end=" ")

        # 根据当前中心点, 构建聚类索引
        clusters = [[] for _ in range(k)]
        for pI, p in enumerate(X):
            c, _ = closest_center(p, cents)
            clusters[c].append(pI)

        # 保存旧中心点, 用于判断是否收敛
        old_cents = cents.copy()

        # 根据聚类结果计算新的中心点
        for i, cluster in enumerate(clusters):
            c = np.mean(X[cluster], axis=0)
            cents[i] = c

        # 如果收敛，则停止更新
        bias = np.sum(np.abs(cents - old_cents))
        print(f"误差为{bias:.2f}")
        if bias < 1e-7:
            break

    # 返回聚类标签结果
    labels = np.zeros(X.shape[0])
    for clusterI, cluster in enumerate(clusters):
        for pI in cluster:
            labels[pI] = clusterI
    return labels, cents


k = 6
X = np.random.rand(200, 2)
labels, cents = Mykmeans(X, k, 20, isKpp=True)

skl_kmeans = KMeans(n_clusters=k, init="k-means++").fit(X)
skl_labels = skl_kmeans.labels_
skl_cents = skl_kmeans.cluster_centers_

fig = make_subplots(rows=2, cols=1)

# 打印自实现的结果
fig.add_trace(go.Scatter(x=X[:, 0], y=X[:, 1], mode="markers", marker=dict(color=labels)), row=1, col=1)
fig.add_trace(go.Scatter(x=cents[:, 0], y=cents[:, 1], mode="markers", marker=dict(color="orange", symbol="star", size=20)), row=1, col=1)

# 打印sklearn的结果
fig.add_trace(go.Scatter(x=X[:, 0], y=X[:, 1], mode="markers", marker=dict(color=skl_labels)), row=2, col=1)
fig.add_trace(go.Scatter(x=skl_cents[:, 0], y=skl_cents[:, 1], mode="markers", marker=dict(color="orange", symbol="star", size=20)), row=2, col=1)

fig.update_layout(title="Kmeans", xaxis_title="X", yaxis_title="Y", dragmode="pan")
fig.show(config={"scrollZoom": True})
```